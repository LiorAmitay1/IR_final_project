{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"a00e032c"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"id":"5ac36d3a","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-d522  GCE       4                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"d8f56ecd","metadata":{"id":"d8f56ecd","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import numpy as np\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"38a897f2","metadata":{"id":"38a897f2","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar 10 20:55 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"72bed56b","metadata":{"id":"72bed56b","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-d522-m.us-central1-a.c.echzor3.internal:43245\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f18fc5b4b50>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":null,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'bucket_209470236' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index"]},{"cell_type":"code","execution_count":8,"id":"e4c523e7","metadata":{"id":"e4c523e7","outputId":"22251c5b-37e6-4632-8776-efee2625495f","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs_title = parquetFile.select(\"title\", \"id\").rdd\n","doc_text_pairs_anchor = parquetFile.select(\"anchor_text\", \"id\").rdd\n","doc_text_pairs_body = parquetFile.select(\"text\", \"id\").rdd\n"]},{"cell_type":"code","execution_count":9,"id":"121fe102","metadata":{"id":"121fe102","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":10,"id":"a5a6f481","metadata":{},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":11,"id":"8f261793","metadata":{},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":null,"id":"f3ad8fea","metadata":{"id":"f3ad8fea","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def tokenize_and_count(text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    return len(tokens)\n","\n","def word_count(text, id):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    counter = Counter(tokens)\n","    tf_pairs = [(token, (id, count)) for token, count in counter.items() if token not in all_stopwords]\n","    return tf_pairs\n","\n","def reduce_word_counts(unsorted_pl):\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","    return sorted_pl\n","\n","def calculate_df(postings):\n","    df_rdd = postings.map(lambda x: (x[0], len(x[1])))\n","    return df_rdd\n","\n","def partition_postings_and_write(postings):\n","    partitioned_postings = postings.groupBy(lambda x: token2bucket_id(x[0]))\n","    posting_locations = partitioned_postings.map(lambda x: InvertedIndex.write_a_posting_list(x, 'title_real_index' ,bucket_name))\n","    return posting_locations\n"]},{"cell_type":"markdown","id":"4c270076","metadata":{},"source":["# title inverted index"]},{"cell_type":"code","execution_count":null,"id":"c0c764dd","metadata":{},"outputs":[],"source":["# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_text_pairs_title.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","id_with_length = doc_text_pairs_title.map(lambda doc: (doc[1], tokenize_and_count(doc[0])))\n","doc_lengths_dict = id_with_length.collectAsMap()\n","\n","total_sum = 0\n","for value in doc_lengths_dict.values():\n","    total_sum += value\n","# total = doc_lengths_dict.map(lambda x: x[1]).mean()\n","avg = total_sum/len(doc_lengths_dict)\n","\n","\n","# filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":null,"id":"ab3296f4","metadata":{"id":"ab3296f4","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","# for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","for blob in client.list_blobs(bucket_name):\n","# for blob in client.list_blobs(bucket_name, prefix='title_real_index'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","            \n","#TO CHECK NOT EMPTY\n","if not super_posting_locs:\n","    print(\"im empty\")\n","else:\n","    count = 0\n","    for x,y in super_posting_locs.items():\n","        print(x , \":\" , y)\n","        count += 1\n","        if count == 5:\n","            break"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"f6f66e3a"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":null,"id":"a5d2cfb6","metadata":{"id":"a5d2cfb6"},"outputs":[],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","inverted.num_of_doc = parquetFile.count()\n","inverted.avg = avg\n","inverted.dict_docID_countWord = doc_lengths_dict\n","\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'title_index',bucket_name)\n","# upload to gs\n","index_src = \"gs://bucket_318820123/title_index.pkl\"\n","index_dst = f'gs://{bucket_name}/title_real_index/title_index.pkl'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"8f880d59","metadata":{"id":"8f880d59","nbgrader":{"grade":false,"grade_id":"cell-index_dst_size","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"2fa031fb","metadata":{},"source":["# text inverted index"]},{"cell_type":"code","execution_count":null,"id":"b7f035b4","metadata":{},"outputs":[],"source":["word_counts1 = doc_text_pairs_body.flatMap(lambda x: word_count(x[0], x[1]))\n","# word_counts1.take(1)\n","postings1 = word_counts1.groupByKey().mapValues(reduce_word_counts)\n","# print(\"ll\")"]},{"cell_type":"code","execution_count":null,"id":"d19b362f","metadata":{},"outputs":[],"source":["id_with_length = doc_text_pairs_body.map(lambda doc: (doc[1], tokenize_and_count(doc[0])))\n","doc_lengths_dict = id_with_length.collectAsMap()\n"]},{"cell_type":"code","execution_count":null,"id":"38006095","metadata":{},"outputs":[],"source":["total_sum = 0\n","for value in doc_lengths_dict.values():\n","    total_sum += value\n","# total = doc_lengths_dict.map(lambda x: x[1]).mean()\n","avg = total_sum/len(doc_lengths_dict)\n"]},{"cell_type":"code","execution_count":null,"id":"9e06bd23","metadata":{},"outputs":[],"source":["# filtering postings and calculate df\n","postings_filtered = postings1.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","# index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":null,"id":"cba9fe2f","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","# for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","for blob in client.list_blobs(bucket_name):\n","# for blob in client.list_blobs(bucket_name, prefix='title_real_index'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","            \n","if not super_posting_locs:\n","    print(\"im empty\")\n","else:\n","    count = 0\n","    for x,y in super_posting_locs.items():\n","        print(x , \":\" , y)\n","        count += 1\n","        if count == 5:\n","            break"]},{"cell_type":"code","execution_count":null,"id":"dff6a12e","metadata":{},"outputs":[],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","inverted.num_of_doc = parquetFile1.count()\n","inverted.avg = avg\n","inverted.dict_docID_countWord = doc_lengths_dict\n","\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'text_index',bucket_name)\n","# upload to gs\n","index_src = \"gs://bucket_209470236/text_index.pkl\"\n","index_dst = f'gs://{bucket_name}/text_real_index/text_index.pkl'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"1b8cb604","metadata":{},"outputs":[],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"aaac2cd3","metadata":{},"source":["# stemming inverted index for title\n"]},{"cell_type":"code","execution_count":18,"id":"f8d68f8c","metadata":{},"outputs":[],"source":["import math\n","import statistics\n","bucket_name = 'bucket_209470236'\n","stemmer = PorterStemmer()\n","parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs_title = parquetFile.select(\"title\", \"id\").rdd\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def tokenize_and_count(text):\n","    tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    return len(tokens)\n","\n","def word_count_stemming(text, id):\n","    \n","    tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    map = {}\n","    for w in tokens:\n","        if w in all_stopwords:\n","            continue\n","        \n","        map[w] = map.get(w, 0) + 1\n","    return [(k,(id,v)) for k,v in map.items()]\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","    return sorted_pl\n","\n","def calculate_df(postings):\n","    df_rdd = postings.map(lambda x: (x[0], len(x[1])))\n","    return df_rdd\n","\n","def partition_postings_and_write(postings):\n","    partitioned_postings = postings.groupBy(lambda x: token2bucket_id(x[0]))\n","    posting_locations = partitioned_postings.map(lambda x: InvertedIndex.write_a_posting_list(x, 'text_real_index_stemming' ,bucket_name))\n","    return posting_locations"]},{"cell_type":"code","execution_count":13,"id":"62817b4b","metadata":{},"outputs":[],"source":["word_counts_stemming = doc_text_pairs_title.flatMap(lambda x: word_count_stemming(x[0], x[1]))\n","# word_counts1.take(1)\n","postings_stemming = word_counts_stemming.groupByKey().mapValues(reduce_word_counts)\n","# print(\"ll\")"]},{"cell_type":"code","execution_count":16,"id":"20a23b6c","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[('foster', (4045403, 31)),\n"," ('air', (4045403, 51)),\n"," ('forc', (4045403, 27)),\n"," ('base', (4045403, 31)),\n"," ('1941', (4045403, 15))]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["word_counts_stemming.take(5)"]},{"cell_type":"code","execution_count":14,"id":"768c8bd2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["id_with_length = doc_text_pairs_title.map(lambda doc: (doc[1], tokenize_and_count(doc[0])))\n","doc_lengths_dict = id_with_length.collectAsMap()\n"]},{"cell_type":"code","execution_count":15,"id":"de77ea6d","metadata":{},"outputs":[],"source":["total_sum = 0\n","for value in doc_lengths_dict.values():\n","    total_sum += value\n","# total = doc_lengths_dict.map(lambda x: x[1]).mean()\n","avg = total_sum/len(doc_lengths_dict)\n"]},{"cell_type":"code","execution_count":16,"id":"0e2ae2af","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/09 16:08:24 WARN TaskSetManager: Lost task 4.0 in stage 7.0 (TID 562) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:26 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 559) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:26 WARN TaskSetManager: Lost task 2.0 in stage 7.0 (TID 557) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:27 WARN TaskSetManager: Lost task 7.0 in stage 7.0 (TID 561) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 18): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:27 WARN TaskSetManager: Lost task 3.0 in stage 7.0 (TID 563) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:28 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 558) (cluster-7ea3-w-1.us-central1-a.c.echzor3.internal executor 20): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000021/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000021/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000021/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:31 WARN TaskSetManager: Lost task 12.0 in stage 7.0 (TID 560) (cluster-7ea3-w-1.us-central1-a.c.echzor3.internal executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000015/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000015/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000015/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:35 WARN TaskSetManager: Lost task 5.1 in stage 7.0 (TID 572) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:38 WARN TaskSetManager: Lost task 2.2 in stage 7.0 (TID 575) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:38 WARN TaskSetManager: Lost task 3.1 in stage 7.0 (TID 573) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:39 WARN TaskSetManager: Lost task 7.2 in stage 7.0 (TID 574) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 18): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:39 WARN TaskSetManager: Lost task 1.2 in stage 7.0 (TID 576) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:43 ERROR TaskSetManager: Task 7 in stage 7.0 failed 4 times; aborting job\n","24/03/09 16:08:43 WARN TaskSetManager: Lost task 15.2 in stage 7.0 (TID 585) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): TaskKilled (Stage cancelled)\n","[Stage 7:>                                                        (0 + 6) / 124]\r"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 7.0 failed 4 times, most recent failure: Lost task 7.3 in stage 7.0 (TID 583) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_17692/2784816226.py\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mw2df_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mw2df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# partition posting lists and write out\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpartition_postings_and_write\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpostings_filtered\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;31m# index_const_time = time() - t_start\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1195\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 7.0 failed 4 times, most recent failure: Lost task 7.3 in stage 7.0 (TID 583) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"]}],"source":["# filtering postings and calculate df\n","postings_filtered = postings_stemming.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n"]},{"cell_type":"code","execution_count":19,"id":"f58188b4","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","# index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":20,"id":"7184c618","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["eun-bi : [('text_real_index_stemming/0_000.bin', 0)]\n","kenli : [('text_real_index_stemming/0_000.bin', 984)]\n","argentino : [('text_real_index_stemming/0_000.bin', 1926)]\n","vandamm : [('text_real_index_stemming/0_000.bin', 25200)]\n","ocsp : [('text_real_index_stemming/0_000.bin', 26322)]\n"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","            \n","if not super_posting_locs:\n","    print(\"im empty\")\n","else:\n","    count = 0\n","    for x,y in super_posting_locs.items():\n","        print(x , \":\" , y)\n","        count += 1\n","        if count == 5:\n","            break"]},{"cell_type":"code","execution_count":27,"id":"d1599de3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying gs://bucket_20943188/text_index_stemming.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][ 60.2 MiB/ 60.2 MiB]                                                \n","Operation completed over 1 objects/60.2 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","inverted.num_of_doc = parquetFile.count()\n","inverted.avg = avg\n","inverted.dict_docID_countWord = doc_lengths_dict\n","bucket_name = 'bucket_209470236'\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'title_index_stemming',bucket_name)\n","# upload to gs\n","index_src = f'gs://{bucket_name}/title_index_stemming.pkl'\n","index_dst = f'gs://{bucket_name}/title_real_index_stemming/title_index_stemming.pkl'\n","!gsutil cp $index_src $index_dst\n","\n","\n","# inverted_title.write_index('.', 'title_index')\n","\n","# # upload to gs\n","# index_src = \"title_index.pkl\"\n","# index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n","# !gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":28,"id":"b7658cbe","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 60.24 MiB  2024-03-09T16:46:12Z  gs://bucket_20943188/text_real_index_stemming/text_index_stemming.pkl\r\n","TOTAL: 1 objects, 63168870 bytes (60.24 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"ad8e175c","metadata":{},"source":["# stemming inverted index for text\n"]},{"cell_type":"code","execution_count":18,"id":"b181bf9d","metadata":{},"outputs":[],"source":["import math\n","import statistics\n","bucket_name = 'bucket_20943188'\n","stemmer = PorterStemmer()\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def tokenize_and_count(text):\n","    tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    return len(tokens)\n","\n","def word_count_stemming(text, id):\n","    \n","    tokens = [stemmer.stem(token.group()) for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    map = {}\n","    for w in tokens:\n","        if w in all_stopwords:\n","            continue\n","        \n","        map[w] = map.get(w, 0) + 1\n","    return [(k,(id,v)) for k,v in map.items()]\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","    sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n","    return sorted_pl\n","\n","def calculate_df(postings):\n","    df_rdd = postings.map(lambda x: (x[0], len(x[1])))\n","    return df_rdd\n","\n","def partition_postings_and_write(postings):\n","    partitioned_postings = postings.groupBy(lambda x: token2bucket_id(x[0]))\n","    posting_locations = partitioned_postings.map(lambda x: InvertedIndex.write_a_posting_list(x, 'text_real_index_stemming' ,bucket_name))\n","    return posting_locations"]},{"cell_type":"code","execution_count":13,"id":"27f96b4a","metadata":{},"outputs":[],"source":["word_counts_stemming = doc_text_pairs_body.flatMap(lambda x: word_count_stemming(x[0], x[1]))\n","# word_counts1.take(1)\n","postings_stemming = word_counts_stemming.groupByKey().mapValues(reduce_word_counts)\n","# print(\"ll\")"]},{"cell_type":"code","execution_count":16,"id":"7f87eeb7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["[('foster', (4045403, 31)),\n"," ('air', (4045403, 51)),\n"," ('forc', (4045403, 27)),\n"," ('base', (4045403, 31)),\n"," ('1941', (4045403, 15))]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["word_counts_stemming.take(5)"]},{"cell_type":"code","execution_count":14,"id":"fdd34bb5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["id_with_length = doc_text_pairs_body.map(lambda doc: (doc[1], tokenize_and_count(doc[0])))\n","doc_lengths_dict = id_with_length.collectAsMap()\n"]},{"cell_type":"code","execution_count":15,"id":"f29192e9","metadata":{},"outputs":[],"source":["total_sum = 0\n","for value in doc_lengths_dict.values():\n","    total_sum += value\n","# total = doc_lengths_dict.map(lambda x: x[1]).mean()\n","avg = total_sum/len(doc_lengths_dict)\n"]},{"cell_type":"code","execution_count":16,"id":"dbd6ba0b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/09 16:08:24 WARN TaskSetManager: Lost task 4.0 in stage 7.0 (TID 562) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:26 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 559) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:26 WARN TaskSetManager: Lost task 2.0 in stage 7.0 (TID 557) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:27 WARN TaskSetManager: Lost task 7.0 in stage 7.0 (TID 561) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 18): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:27 WARN TaskSetManager: Lost task 3.0 in stage 7.0 (TID 563) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:28 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 558) (cluster-7ea3-w-1.us-central1-a.c.echzor3.internal executor 20): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000021/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000021/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000021/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:31 WARN TaskSetManager: Lost task 12.0 in stage 7.0 (TID 560) (cluster-7ea3-w-1.us-central1-a.c.echzor3.internal executor 14): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000015/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000015/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000015/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:35 WARN TaskSetManager: Lost task 5.1 in stage 7.0 (TID 572) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:38 WARN TaskSetManager: Lost task 2.2 in stage 7.0 (TID 575) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000020/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:38 WARN TaskSetManager: Lost task 3.1 in stage 7.0 (TID 573) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 16): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000017/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:39 WARN TaskSetManager: Lost task 7.2 in stage 7.0 (TID 574) (cluster-7ea3-w-0.us-central1-a.c.echzor3.internal executor 18): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000019/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:39 WARN TaskSetManager: Lost task 1.2 in stage 7.0 (TID 576) (cluster-7ea3-w-3.us-central1-a.c.echzor3.internal executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n","    process()\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n","    serializer.dump_stream(out_iter, outfile)\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n","    vs = list(itertools.islice(iterator, batch))\n","  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 222, in write_a_posting_list\n","    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 45, in __init__\n","    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n","  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000016/inverted_index_gcp.py\", line 22, in get_bucket\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n","    return Bucket(client=self, name=bucket_name, user_project=user_project)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n","    name = _validate_name(name)\n","  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n","    if not all([name[0].isalnum(), name[-1].isalnum()]):\n","IndexError: string index out of range\n","\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n","\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n","\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n","\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n","\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n","\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n","\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n","\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n","\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n","\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n","\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n","\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n","\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n","\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n","\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n","\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n","\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","24/03/09 16:08:43 ERROR TaskSetManager: Task 7 in stage 7.0 failed 4 times; aborting job\n","24/03/09 16:08:43 WARN TaskSetManager: Lost task 15.2 in stage 7.0 (TID 585) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): TaskKilled (Stage cancelled)\n","[Stage 7:>                                                        (0 + 6) / 124]\r"]},{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 7.0 failed 4 times, most recent failure: Lost task 7.3 in stage 7.0 (TID 583) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/tmp/ipykernel_17692/2784816226.py\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mw2df_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mw2df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAsMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# partition posting lists and write out\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpartition_postings_and_write\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpostings_filtered\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;31m# index_const_time = time() - t_start\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1195\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1197\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1198\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1199\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 7.0 failed 4 times, most recent failure: Lost task 7.3 in stage 7.0 (TID 583) (cluster-7ea3-w-2.us-central1-a.c.echzor3.internal executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:959)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2314)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_17692/78734506.py\", line 45, in <lambda>\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 222, in write_a_posting_list\n    with closing(MultiFileWriter(base_dir, bucket_id, bucket_name)) as writer:\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 45, in __init__\n    self._bucket = None if bucket_name is None else get_bucket(bucket_name)\n  File \"/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1709979640550_0004/container_1709979640550_0004_01_000018/inverted_index_gcp.py\", line 22, in get_bucket\n    return storage.Client(PROJECT_ID).bucket(bucket_name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/client.py\", line 296, in bucket\n    return Bucket(client=self, name=bucket_name, user_project=user_project)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/bucket.py\", line 667, in __init__\n    name = _validate_name(name)\n  File \"/opt/conda/miniconda3/lib/python3.10/site-packages/google/cloud/storage/_helpers.py\", line 91, in _validate_name\n    if not all([name[0].isalnum(), name[-1].isalnum()]):\nIndexError: string index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2333)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"]}],"source":["# filtering postings and calculate df\n","postings_filtered = postings_stemming.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n"]},{"cell_type":"code","execution_count":19,"id":"27b0b02d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","# index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":20,"id":"51041408","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["eun-bi : [('text_real_index_stemming/0_000.bin', 0)]\n","kenli : [('text_real_index_stemming/0_000.bin', 984)]\n","argentino : [('text_real_index_stemming/0_000.bin', 1926)]\n","vandamm : [('text_real_index_stemming/0_000.bin', 25200)]\n","ocsp : [('text_real_index_stemming/0_000.bin', 26322)]\n"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","# for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","for blob in client.list_blobs(bucket_name):\n","# for blob in client.list_blobs(bucket_name, prefix='title_real_index'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","            \n","if not super_posting_locs:\n","    print(\"im empty\")\n","else:\n","    count = 0\n","    for x,y in super_posting_locs.items():\n","        print(x , \":\" , y)\n","        count += 1\n","        if count == 5:\n","            break"]},{"cell_type":"code","execution_count":27,"id":"616eea79","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying gs://bucket_20943188/text_index_stemming.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][ 60.2 MiB/ 60.2 MiB]                                                \n","Operation completed over 1 objects/60.2 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","inverted.num_of_doc = parquetFile.count()\n","inverted.avg = avg\n","inverted.dict_docID_countWord = doc_lengths_dict\n","bucket_name = 'bucket_20943188'\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","# write the global stats out\n","inverted.write_index('.', 'text_index_stemming',bucket_name)\n","# upload to gs\n","index_src = f'gs://{bucket_name}/text_index_stemming.pkl'\n","index_dst = f'gs://{bucket_name}/text_real_index_stemming/text_index_stemming.pkl'\n","!gsutil cp $index_src $index_dst\n","\n","\n","# inverted_title.write_index('.', 'title_index')\n","\n","# # upload to gs\n","# index_src = \"title_index.pkl\"\n","# index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n","# !gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":28,"id":"4171202d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 60.24 MiB  2024-03-09T16:46:12Z  gs://bucket_20943188/text_real_index_stemming/text_index_stemming.pkl\r\n","TOTAL: 1 objects, 63168870 bytes (60.24 MiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"15033c9e","metadata":{},"source":["# anchor inverted index\n"]},{"cell_type":"code","execution_count":null,"id":"776a9c98","metadata":{},"outputs":[],"source":["word_counts1 = doc_text_pairs_anchor.flatMap(lambda x: word_count(x[0], x[1]))\n","# word_counts1.take(1)\n","postings1 = word_counts1.groupByKey().mapValues(reduce_word_counts)\n","# print(\"ll\")"]},{"cell_type":"code","execution_count":null,"id":"85624043","metadata":{},"outputs":[],"source":["id_with_length = doc_text_pairs_anchor.map(lambda doc: (doc[1], tokenize_and_count(doc[0])))\n","doc_lengths_dict = id_with_length.collectAsMap()\n"]},{"cell_type":"code","execution_count":null,"id":"d0a0de37","metadata":{},"outputs":[],"source":["total_sum = 0\n","for value in doc_lengths_dict.values():\n","    total_sum += value\n","# total = doc_lengths_dict.map(lambda x: x[1]).mean()\n","avg = total_sum/len(doc_lengths_dict)\n"]},{"cell_type":"code","execution_count":null,"id":"ebbd1b0a","metadata":{},"outputs":[],"source":["# filtering postings and calculate df\n","postings_filtered = postings1.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered).collect()\n","# index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":null,"id":"5a2dcc05","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs = defaultdict(list)\n","# for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","for blob in client.list_blobs(bucket_name):\n","# for blob in client.list_blobs(bucket_name, prefix='title_real_index'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","            \n","if not super_posting_locs:\n","    print(\"im empty\")\n","else:\n","    count = 0\n","    for x,y in super_posting_locs.items():\n","        print(x , \":\" , y)\n","        count += 1\n","        if count == 5:\n","            break"]},{"cell_type":"code","execution_count":null,"id":"9bf92558","metadata":{},"outputs":[],"source":["# Create inverted index instance\n","inverted = InvertedIndex()\n","inverted.num_of_doc = parquetFile1.count()\n","inverted.avg = avg\n","inverted.dict_docID_countWord = doc_lengths_dict\n","\n","# Create inverted index instance\n","inverted_anchor = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted_anchor.posting_locs = super_posting_locs_anchor\n","inverted_anchor.df = w2df_anchor\n","# write the global stats out\n","inverted_anchor.write_index('.', 'anchor_index')\n","\n","# upload to gs\n","index_src = \"anchor_index.pkl\"\n","index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"markdown","id":"0fd5ed47","metadata":{},"source":["# count words in doc Dictionary"]},{"cell_type":"code","execution_count":null,"id":"0e5dd120","metadata":{},"outputs":[],"source":["parquetFile1 = spark.read.parquet(*paths)\n","doc_text_pairs1 = parquetFile1.select(\"text\", \"id\").rdd\n","# doc_text_pairs1.take(5)\n","# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py\n","# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index_gcp import InvertedIndex\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","def tokenize_and_count(text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in all_stopwords]\n","    return len(tokens)\n","\n","id_with_length = doc_text_pairs1.map(lambda doc: (doc[1], tokenize_and_count(doc[0])))\n","doc_lengths_dict = id_with_length.collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"b0f3205a","metadata":{},"outputs":[],"source":["# Specify the bucket name and file name in the bucket\n","bucket_name = 'bucket_20943188'\n","file_name = 'dict_docID_countWords.pkl'\n","\n","# Serialize the defaultdict object to a pickle file\n","with open(file_name, 'wb') as f:\n","    pickle.dump(doc_lengths_dict, f)\n","\n","# Upload the pickle file to Google Cloud Storage\n","client = storage.Client()\n","bucket = client.bucket(bucket_name)\n","blob = bucket.blob(file_name)\n","blob.upload_from_filename(file_name)"]},{"cell_type":"markdown","id":"7e77c778","metadata":{},"source":["# title to doc Dictionary"]},{"cell_type":"code","execution_count":null,"id":"48037eef","metadata":{},"outputs":[],"source":["id_title_dict = parquetFile.select(\"id\",\"title\").rdd.collectAsMap()\n","with open(\"id_title_dict.pkl\", 'wb') as f:\n","    pickle.dump(id_title_dict, f)"]},{"cell_type":"code","execution_count":null,"id":"5e681eb5","metadata":{},"outputs":[],"source":["index_src = \"id_title_dict.pkl\"\n","index_dst = f'gs://{bucket_name}/{index_src}'"]},{"cell_type":"code","execution_count":null,"id":"6ca9ed4d","metadata":{},"outputs":[],"source":["!gsutil cp $index_src $index_dst"]},{"cell_type":"markdown","id":"e78ce703","metadata":{},"source":["# Page Rank Dictionary"]},{"cell_type":"code","execution_count":null,"id":"844f6193","metadata":{},"outputs":[],"source":["t_start = time()\n","pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr_time = time() - t_start\n","pr.show()"]},{"cell_type":"code","execution_count":null,"id":"1faa3007","metadata":{},"outputs":[],"source":["#after unzip the file that in the bucket\n","bucket_name = 'bucket_318820123'\n","pr_URL = 'pr_part-00000-2c654e78-7660-4927-9c12-36d456f9ac1f-c000.csv'\n","storage_client = storage.Client()\n","bucket = storage_client.bucket(bucket_name)\n","blob_pr = bucket.blob(pr_URL)\n","contents_pr = blob_pr.download_as_bytes()\n","csv_buffer = io.BytesIO(contents_pr)\n","df = pd.read_csv(csv_buffer, header=None)\n","first_column = df.iloc[:, 0]\n","second_column = df.iloc[:, 1]\n","pr_dict = dict(zip(first_column, second_column))\n","max_pr_value = max(pr_dict.values())\n","norm_pr = {doc_id: view/max_pr_value for doc_id, view in pr_dict.items()}\n","# pr_pkl_url = 'page_rank/'\n","path =  'PageRank/page_rank_dict.pkl'\n","bucket = None if bucket_name is None else get_bucket(bucket_name)\n","with _open(path, 'wb', bucket) as f:\n","      pickle.dump(norm_pr,f)"]},{"cell_type":"markdown","id":"85a2af5d","metadata":{},"source":["# Page views dict"]},{"cell_type":"code","execution_count":null,"id":"bc600744","metadata":{},"outputs":[],"source":["# Paths\n","# Using user page views (as opposed to spiders and automated traffic) for the\n","# month of August 2021\n","pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path)\n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","# Download the file (2.3GB)\n","!wget -N $pv_path\n","# Filter for English pages, and keep just two fields: article ID (3) and monthly\n","# total number of page views (5). Then, remove lines with article id or page\n","# view values that are not a sequence of digits.\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","# Create a Counter (dictionary) that sums up the pages views for the same\n","# article, resulting in a mapping from article id to total page views.\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","    for line in f:\n","        parts = line.split(' ')\n","        wid2pv.update({int(parts[0]): int(parts[1])})\n","# write out the counter as binary file (pickle it)\n","with open(pv_clean, 'wb') as f:\n","    pickle.dump(wid2pv, f)\n","# read in the counter\n","with open(pv_clean, 'rb') as f:\n","    wid2pv = pickle.loads(f.read())"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}